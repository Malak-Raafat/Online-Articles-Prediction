# -*- coding: utf-8 -*-
"""Task4 _datascience.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15bPhVVpbgm5-TWcA2pCwzrAHTN0KUGX1

## Import Libraries & Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.preprocessing import OneHotEncoder

np.random.seed(42)
# %matplotlib inline

df=pd.read_csv('/content/OnlineArticlesPopularity.csv')

df.shape

"""## Preprocessing"""

df.columns

df.hist(figsize = (20 , 20))

df.head()

df.info()

# Remove leading whitespace from column names
df.columns = df.columns.str.strip()

df = df.rename(columns={'channel type': 'channel_type'})

"""###Check For Nulls Or Duplicates"""

null_values=df.isnull().sum()

print(null_values)

duplicate_rows = df.duplicated().sum()

print("Number of duplicate rows:", duplicate_rows)

df.nunique()

unique_values_counts = {}
for col in df.columns:
    if df[col].dtype == 'object':
        unique_values_counts[col] = df[col].nunique()

print("Number of unique values in object columns:")
for col, count in unique_values_counts.items():
    print(f"{col}: {count}")

df['channel_type'].unique()

#channel type is "object" so we found [] -> Null
count_brackets = (df['channel_type'] == '[]').sum()

print(count_brackets)

total_non_null = df['channel_type'].notnull().sum()

percentage_brackets = (count_brackets / total_non_null) * 100

print(percentage_brackets)

df['channel_type'].replace('[]', pd.NA , inplace=True)

df['channel_type'].isnull().sum()

"""As 15% is a big percentage so we are gong to impute it using mode as it is categorical data"""

# Count the number of occurrences of each unique value in the "channel_type" column
channel_type_counts = df['channel_type'].value_counts()

print("Counts of each unique value in the 'channel_type' column:")
print(channel_type_counts)

mode_value = df['channel_type'].mode()[0]

df['channel_type'] = df['channel_type'].fillna(mode_value)

df['channel_type'].isnull().sum()

df['weekday'].unique()

df['isWeekEnd'].unique()

"""### Encoding"""

label_encoder = LabelEncoder()
label_encoder1 = LabelEncoder()

# Encode 'url' column
df['url_encoded'] = label_encoder.fit_transform(df['url'])

# Encode 'title' column
df['title_encoded'] = label_encoder.fit_transform(df['title'])

# Encode 'channel_type' column
df['channel_type_encoded'] = label_encoder.fit_transform(df['channel_type'])

# Encode 'weekday' column
df['weekday_encoded'] = label_encoder1.fit_transform(df['weekday'])

# Create mapping dictionary for 'channel_type'
channel_type_encoded_to_categorical = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))

# Create mapping dictionary for 'weekday'
weekday_encoded_to_categorical = dict(zip(label_encoder1.transform(label_encoder1.classes_), label_encoder1.classes_))

print("Encoded numerical value to original categorical value mapping for 'channel_type':")
for encoded_value, categorical_value in channel_type_encoded_to_categorical.items():
    print(f"{encoded_value}: {categorical_value}")

print("\nEncoded numerical value to original categorical value mapping for 'weekday':")
for encoded_value, categorical_value in weekday_encoded_to_categorical.items():
    print(f"{encoded_value}: {categorical_value}")

one_hot_encoder = OneHotEncoder()

# Encode 'isWeekEnd' column
isWeekEnd_encoded = one_hot_encoder.fit_transform(df[['isWeekEnd']]).toarray()

# Create new columns for each category
df[['isWeekEnd_0', 'isWeekEnd_1']] = isWeekEnd_encoded

df.columns

"""### EDA"""

# List of columns to plot
columns_to_plot = ['isWeekEnd', 'weekday', 'channel_type']

# Set up the figure and axes
fig, axes = plt.subplots(nrows=len(columns_to_plot), ncols=1, figsize=(14, 6 * len(columns_to_plot)))

# Loop through each column to create bar plots
for i, column in enumerate(columns_to_plot):
    # Count the occurrences of 'yes' and 'no' in the specific column
    value_counts = df[column].value_counts()

    # Create bar plot
    sns.barplot(x=value_counts.index, y=value_counts.values, palette='pastel', ax=axes[i])

    # Set labels and title
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'Bar Plot of {column}')

# Adjust layout
plt.tight_layout()

# Show plots
plt.show()

columns_to_exclude = ['channel_type_encoded', 'channel type']

columns_to_plot = [col for col in df.columns if col not in columns_to_exclude]

fig, axes = plt.subplots(nrows=len(columns_to_plot), ncols=1, figsize=(8, 6*len(columns_to_plot)))

# Create histograms for each column
for i, column in enumerate (columns_to_plot):
    ax = axes[i]
    ax.hist(df[column], bins=20, color='pink', edgecolor='black')  # Adjust the number of bins as needed
    ax.set_title(column)
    ax.set_xlabel('Value')
    ax.set_ylabel('Frequency')
    ax.grid(False)

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()
'''

channel_type_counts = df['channel_type'].value_counts()

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(channel_type_counts, labels=channel_type_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Channel Types')
plt.axis('equal')
plt.show()

column_names1 = ['kw_min_min', 'kw_max_min', 'kw_avg_min']
column_stats1 = df[column_names1].describe().transpose()
# Set up the figure and axis
plt.figure(figsize=(12, 8))

# Plot mean values
plt.subplot(3, 1, 1)
sns.barplot(x=column_stats1.index, y='mean', data=column_stats1, color='blue')
plt.ylabel('Mean')
plt.title('Mean ')

# Plot median values
plt.subplot(3, 1, 2)
sns.barplot(x=column_stats1.index, y='50%', data=column_stats1, color='orange')
plt.ylabel('Median')
plt.title('Median ')

# Plot standard deviation values
plt.subplot(3, 1, 3)
sns.barplot(x=column_stats1.index, y='std', data=column_stats1, color='green')
plt.ylabel('Standard Deviation')
plt.title('Standard ')

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()
'''

# Correct column names without leading and trailing spaces
column_names2 = ['kw_min_max', 'kw_max_max', 'kw_avg_max']

# Calculate summary statistics for the columns
column_stats2 = df[column_names2].describe().transpose()

# Set up the figure and axis
plt.figure(figsize=(12, 8))

# Plot mean values
plt.subplot(3, 1, 1)
sns.barplot(x=column_stats2.index, y='mean', data=column_stats2, color='blue')
plt.ylabel('Mean')
plt.title('Mean')

# Plot median values
plt.subplot(3, 1, 2)
sns.barplot(x=column_stats2.index, y='50%', data=column_stats2, color='orange')
plt.ylabel('Median')
plt.title('Median')

# Plot standard deviation values
plt.subplot(3, 1, 3)
sns.barplot(x=column_stats2.index, y='std', data=column_stats2, color='green')
plt.ylabel('Standard Deviation')
plt.title('Standard Deviation')

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

column_names3 = ['kw_min_avg', 'kw_max_avg', 'kw_avg_avg']
column_stats3 = df[column_names3].describe().transpose()
# Set up the figure and axis
plt.figure(figsize=(12, 8))

# Plot mean values
plt.subplot(3, 1, 1)
sns.barplot(x=column_stats3.index, y='mean', data=column_stats3, color='blue')
plt.ylabel('Mean')
plt.title('Mean')

# Plot median values
plt.subplot(3, 1, 2)
sns.barplot(x=column_stats3.index, y='50%', data=column_stats3, color='orange')
plt.ylabel('Median')
plt.title('Median')

# Plot standard deviation values
plt.subplot(3, 1, 3)
sns.barplot(x=column_stats3.index, y='std', data=column_stats3, color='green')
plt.ylabel('Standard Deviation')
plt.title('Standard Deviation ')

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

column_names4 = ['self_reference_min_shares','self_reference_max_shares', 'self_reference_avg_sharess']
column_stats4 = df[column_names4].describe().transpose()
# Set up the figure and axis
plt.figure(figsize=(12, 8))

# Plot mean values
plt.subplot(3, 1, 1)
sns.barplot(x=column_stats4.index, y='mean', data=column_stats4, color='blue')
plt.ylabel('Mean')
plt.title('Mean')

# Plot median values
plt.subplot(3, 1, 2)
sns.barplot(x=column_stats4.index, y='50%', data=column_stats4, color='orange')
plt.ylabel('Median')
plt.title('Median')

# Plot standard deviation values
plt.subplot(3, 1, 3)
sns.barplot(x=column_stats4.index, y='std', data=column_stats4, color='green')
plt.ylabel('Standard Deviation')
plt.title('Standard Deviation ')

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Define columns and colors
columns = {
    'Negative Polarity': ['avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity'],
    'Positive Polarity': ['avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity']
}
colors = ['purple', 'grey', 'green']

# Set up the figure and axes
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12))

# Loop through each polarity type and its columns
for i, (polarity, col_names) in enumerate(columns.items()):
    column_stats = df[col_names].describe().transpose()

    # Loop through each statistic and plot
    for j, stat in enumerate(['mean', '50%', 'std']):
        sns.barplot(x=column_stats.index, y=stat, data=column_stats, color=colors[j], ax=axes[j, i])
        axes[j, i].set_ylabel(stat.capitalize())
        axes[j, i].set_title(stat.capitalize())
        axes[j, i].set_xlabel(polarity)

# Adjust layout
plt.tight_layout()

# Show plots
plt.show()

# Define columns and colors
columns = {
    'Negative Words': ['global_rate_negative_words', 'rate_negative_words'],
    'Positive Words': ['global_rate_positive_words', 'rate_positive_words']
}
colors = ['purple', 'grey', 'green']

# Set up the figure and axes
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 12))

# Loop through each polarity type and its columns
for i, (polarity, col_names) in enumerate(columns.items()):
    column_stats = df[col_names].describe().transpose()

    # Loop through each statistic and plot
    for j, stat in enumerate(['mean', '50%', 'std']):
        sns.barplot(x=column_stats.index, y=stat, data=column_stats, color=colors[j], ax=axes[j, i])
        axes[j, i].set_ylabel(stat.capitalize())
        axes[j, i].set_title(stat.capitalize())
        axes[j, i].set_xlabel(polarity)

# Adjust layout
plt.tight_layout()

# Show plots
plt.show()

df.columns

import matplotlib.pyplot as plt

# Define the columns for the boxplots
columns = [
       'timedelta', 'n_tokens_content', 'n_unique_tokens',
       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',
       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',
       'num_keywords', 'kw_min_min', 'kw_max_min',
       'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg',
       'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares',
       'self_reference_max_shares', 'self_reference_avg_sharess',
       'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04',
       'global_subjectivity', 'global_sentiment_polarity',
       'global_rate_positive_words', 'global_rate_negative_words',
       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',
       'min_positive_polarity', 'max_positive_polarity',
       'avg_negative_polarity', 'min_negative_polarity',
       'max_negative_polarity', 'shares', 'url_encoded', 'title_encoded',
       'channel_type_encoded', 'weekday_encoded', 'isWeekEnd_0',
       'isWeekEnd_1'
]

# Set up the figure and axes
fig, axes = plt.subplots(4, 4, figsize=(20, 20))

# Flatten the axes array to iterate over each subplot
axes = axes.flatten()

# Loop through each column and plot a boxplot
for i, col in enumerate(columns):
    if i < len(axes):  # Check if the index is within the range of axes
        axes[i].boxplot(df[col], vert=True)
        axes[i].set_title(f'Boxplot of {col.strip().capitalize()}')  # Strip leading spaces and capitalize the column name
        axes[i].set_ylabel('')  # Remove y-label for cleaner presentation

# Adjust layout
plt.tight_layout()

# Show plots
plt.show()

df.columns

"""### Feature Selection"""

X = df.drop(['shares','channel_type','channel_type_encoded','url','url_encoded','title_encoded','title','weekday_encoded','weekday','isWeekEnd_0',
       'isWeekEnd_1','isWeekEnd'],axis=1)
y = df['shares']

X.shape

"""#### Information Gain"""

# Calculate information gains
information_gains = mutual_info_regression(X, y)

# Create a dictionary mapping features to their information gains
feature_information_gains = dict(zip(X.columns, information_gains))

# Sort features by their information gains in descending order
sorted_features = sorted(feature_information_gains.items(), key=lambda x: x[1], reverse=True)
# Print features and their information gains
for feature, gain in sorted_features:
    print(f"{feature}: {gain}")
# Extract features and gains for plotting
features, gains = zip(*sorted_features)

# Set up the figure and axes
plt.figure(figsize=(12, 8))

# Create bar plot
sns.barplot(x=gains, y=features, palette='viridis')

# Set labels and title
plt.xlabel('Information Gain')
plt.ylabel('Feature')
plt.title('Information Gain of Features')

# Show plot
plt.show()

top_features_mutual_info = [feature for feature, gain in sorted_features[:20]]

top_features_mutual_info

"""#### Random Forest Regressor"""

# Initialize Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)

rf.fit(X, y)

feature_importances = rf.feature_importances_

feat_importances = pd.Series(feature_importances, index=X.columns)

feat_importances = feat_importances.sort_values(ascending=False)


plt.figure(figsize=(20, 15))
feat_importances.plot(kind='barh', color='teal')
plt.title('Random Forest Feature Importance')
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.show()

# Print sorted feature importances
print("Sorted Feature Importances:")
print(feat_importances)

top_features_random_forest = [feature for feature, gain in sorted_features[:20]]

top_features_random_forest

"""#### Gradient Boosting"""

from sklearn.ensemble import GradientBoostingRegressor

boosting = GradientBoostingRegressor(n_estimators=100, random_state=42)

boosting.fit(X, y)

feature_importances_boosting = boosting.feature_importances_

feature_importances_boosting_dict = dict(zip(X.columns, feature_importances_boosting))

sorted_features_boosting = sorted(feature_importances_boosting_dict.items(), key=lambda x: x[1], reverse=True)

for feature, importance in sorted_features_boosting:
    print(f"{feature}: {importance}")



# Extracting feature names and importances for plotting
features = [f[0] for f in sorted_features_boosting]
importances = [f[1] for f in sorted_features_boosting]

# Creating the bar plot
plt.figure(figsize=(12, 8))
plt.bar(features, importances)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importances - Gradient Boosting Regressor')
plt.xticks(rotation=45, ha='right')
plt.show()

top_features_boosting = [feature for feature, importance in sorted_features_boosting[:20]]

top_features_boosting

"""#### Correlation Matrix"""

# Create the correlation matrix
dfCorr = pd.concat([X, y], axis=1)
correlation_matrix = dfCorr.corr()

# Print correlations with the target variable
print("\nCorrelations with the Target Variable 'shares':")
print(correlation_matrix['shares'].sort_values(ascending=False))

# Generate the heatmap
plt.figure(figsize=(30, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.show()

abs_correlation = correlation_matrix['shares'].abs().sort_values(ascending=False)

# Extract top 20 features based on absolute correlation with the target variable
top_features_abs_correlation = abs_correlation.index[1:21].tolist()

top_features_abs_correlation

all_top_features = set(top_features_mutual_info + top_features_boosting + top_features_abs_correlation + top_features_random_forest)

feature_counts = {feature: sum(feature in top_feature_list for top_feature_list in [top_features_mutual_info, top_features_boosting, top_features_abs_correlation, top_features_random_forest]) for feature in all_top_features}

selected_features = [feature for feature, count in feature_counts.items() if count > 2]

print("Selected Features:")
for feature in selected_features:
    print(feature)

"""#### Standralization
As it is useful when the features in the dataset have varying scales
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit and transform the data
data_scaled = scaler.fit_transform(df[selected_features])

# Convert the scaled data back to a DataFrame
data_scaled_df = pd.DataFrame(data_scaled, columns=selected_features)

# Convert the scaled data back to a DataFrame
data_scaled_df = df[selected_features]

data_scaled_df

shares_data = df[['shares']]

# Initialize StandardScaler
scaler = StandardScaler()

# Fit scaler to the data and transform
df['standardized_shares'] = scaler.fit_transform(shares_data)

df['standardized_shares']

"""#### ANOVA"""

model = ols('shares ~ C(channel_type)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

model = ols('shares ~ C(weekday)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

model = ols('shares ~ C(isWeekEnd)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

"""The small p-value indicates that the categorical variables are statistically significant in explaining the variability in the target variable 'shares'. Therefore, it suggests that they are likely to have an effect on the number of shares.

**Conclusion**
- Numerical Features


1. num_videos
2. self_reference_min_shares
3. kw_max_min
4. kw_min_avg
5. kw_avg_max
6. LDA_03
7. LDA_02
8. LDA_00
9. LDA_01
10. rate_positive_words
11. kw_max_avg
12. self_reference_max_shares
13. kw_avg_avg
14. self_reference_avg_sharess

- Categorical Features:
1. channel_type
2. weekday
3. isWeekEnd

## Modeling
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.linear_model import Lasso
from sklearn.model_selection import cross_val_score

X = pd.concat([data_scaled_df, df[['channel_type_encoded', 'weekday_encoded', 'isWeekEnd_0', 'isWeekEnd_1']]], axis=1)
y=df['standardized_shares']

X.shape

X.columns

X_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.2 , random_state= 42 , shuffle = True)

"""#### Linear Regression"""

#Step 1: Train Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 2: Make predictions
y_pred = model.predict(X_test)

# Step 3: Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Step 4: Print evaluation metrics
print(f"MSE: {mse}")
print(f"MAE: {mae}")

for column in X_test.columns:
    # Plot the actual data points
    plt.scatter(X_test[column], y_test, color='blue', label='Actual')

    # Plot the regression line
    plt.plot(X_test[column], y_pred, color='pink', label='Linear Regression')

    plt.title(f'Linear Regression for {column}')
    plt.xlabel(column)
    plt.ylabel('y')
    plt.legend()
    plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Exclude the last 4 columns
X_test_subset = X_test.iloc[:, :-4]

# Define the number of columns and rows for subplots
num_cols = len(X_test_subset.columns)  # Exclude the last 4 columns
num_rows = int(np.ceil(num_cols / 5))  # Number of rows to accommodate 5 plots per row

# Create subplots
fig, axs = plt.subplots(num_rows, 5, figsize=(20, 4 * num_rows))

for i, column in enumerate(X_test_subset.columns):
    # Calculate the row and column index for the current subplot
    row_index = i // 5
    col_index = i % 5

    # Plot the actual data points
    axs[row_index, col_index].scatter(X_test_subset[column], y_test, color='blue', label='Actual')

    # Plot the regression line
    axs[row_index, col_index].plot(X_test_subset[column], y_pred, color='pink', label='Linear Regression')

    axs[row_index, col_index].set_title(f'Linear Regression for {column}')
    axs[row_index, col_index].set_xlabel(column)
    axs[row_index, col_index].set_ylabel('y')
    axs[row_index, col_index].legend()

# Hide empty subplots
for i in range(num_cols, num_rows * 5):
    axs.flatten()[i].axis('off')

plt.tight_layout()
plt.show()

"""#### Cross Validation"""

# Step 1: Initialize Linear Regression model
model = LinearRegression()

# Step 2: Perform Cross Validation for MSE
mse_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Step 3: Print MSE scores for each fold
print("MSE scores for each fold:")
for i, mse_score in enumerate(mse_scores):
    print(f"Fold {i+1}: {mse_score}")

# Step 4: Calculate mean MSE
mean_mse = mse_scores.mean()
print(f"\nMean MSE: {mean_mse}")

# Step 5: Perform Cross Validation for MAE
mae_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')

# Step 6: Print MAE scores for each fold
print("\nMAE scores for each fold:")
for i, mae_score in enumerate(mae_scores):
    print(f"Fold {i+1}: {mae_score}")

# Step 7: Calculate mean MAE
mean_mae = mae_scores.mean()
print(f"\nMean MAE: {mean_mae}")

"""### Lasso Regression"""

# Step 1: Train Lasso Regression model
lasso_model = Lasso(alpha=0.1, random_state=42)
lasso_model.fit(X_train, y_train)

# Step 2: Make predictions
y_pred = lasso_model.predict(X_test)

# Step 3: Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Step 4: Print evaluation metrics
print(f"MSE: {mse}")
print(f"MAE: {mae}")

num_cols = len(X_test.columns) - 4  # Exclude the last 4 columns
num_rows = int(np.ceil(num_cols / 5))  # Number of rows to accommodate 5 plots per row

fig, axs = plt.subplots(num_rows, 5, figsize=(20, 4 * num_rows))

for i, column in enumerate(X_test.columns[:-4]):
    # Calculate the row and column index for the current subplot
    row_index = i // 5
    col_index = i % 5

    # Plot the actual data points
    axs[row_index, col_index].scatter(X_test[column], y_test, color='blue', label='Actual')

    # Plot the regression line
    axs[row_index, col_index].plot(X_test[column], y_pred, color='pink', label='Linear Regression')

    axs[row_index, col_index].set_title(f'Linear Regression for {column}')
    axs[row_index, col_index].set_xlabel(column)
    axs[row_index, col_index].set_ylabel('y')
    axs[row_index, col_index].legend()

# Hide empty subplots
for i in range(num_cols, num_rows * 5):
    axs.flatten()[i].axis('off')

plt.tight_layout()
plt.show()

for column in X_test.columns:
    # Plot the actual data points
    plt.scatter(X_test[column], y_test, color='blue', label='Actual')

    # Plot the regression line
    plt.plot(X_test[column], y_pred, color='pink', label='Linear Regression')

    plt.title(f'Linear Regression for {column}')
    plt.xlabel(column)
    plt.ylabel('y')
    plt.legend()
    plt.show()

"""#### Cross Validation"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Initialize Lasso Regression model
model = Lasso(alpha=0.1, random_state=42)

# Step 2: Perform Cross Validation for MSE
mse_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Step 3: Print MSE scores for each fold
print("MSE scores for Lasso Regression:")
for i, mse_score in enumerate(mse_scores):
    print(f"Fold {i+1}: {mse_score}")

# Step 4: Calculate mean MSE
mean_mse = mse_scores.mean()
print(f"\nMean MSE for Lasso Regression: {mean_mse}")

# Step 5: Perform Cross Validation for MAE
mae_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')

# Step 6: Print MAE scores for each fold
print("\nMAE scores for Lasso Regression:")
for i, mae_score in enumerate(mae_scores):
    print(f"Fold {i+1}: {mae_score}")

# Step 7: Calculate mean MAE
mean_mae = mae_scores.mean()
print(f"\nMean MAE for Lasso Regression: {mean_mae}")

"""### XGBoost"""

# Step 1: Train XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=50, learning_rate=0.12, max_depth=4)
xgb_model.fit(X_train, y_train)

# Step 2: Make predictions
y_pred = xgb_model.predict(X_test)

# Step 3: Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Step 4: Print evaluation metrics
print(f"MSE: {mse}")
print(f"MAE: {mae}")

num_cols = len(X_test.columns) - 4  # Exclude the last 4 columns
num_rows = int(np.ceil(num_cols / 5))  # Number of rows to accommodate 5 plots per row

fig, axs = plt.subplots(num_rows, 5, figsize=(20, 4 * num_rows))

for i, column in enumerate(X_test.columns[:-4]):
    # Calculate the row and column index for the current subplot
    row_index = i // 5
    col_index = i % 5

    # Plot the actual data points
    axs[row_index, col_index].scatter(X_test[column], y_test, color='blue', label='Actual')

    # Plot the regression line
    axs[row_index, col_index].plot(X_test[column], y_pred, color='pink', label='Linear Regression')

    axs[row_index, col_index].set_title(f'Linear Regression for {column}')
    axs[row_index, col_index].set_xlabel(column)
    axs[row_index, col_index].set_ylabel('y')
    axs[row_index, col_index].legend()

# Hide empty subplots
for i in range(num_cols, num_rows * 5):
    axs.flatten()[i].axis('off')

plt.tight_layout()
plt.show()

"""#### Cross Validation"""

from sklearn.model_selection import cross_val_score
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Initialize XGBoost model
model = xgb.XGBRegressor()

# Step 2: Perform Cross Validation for MSE
mse_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Step 3: Print MSE scores for each fold
print("MSE scores for XGBoost:")
for i, mse_score in enumerate(mse_scores):
    print(f"Fold {i+1}: {mse_score}")

# Step 4: Calculate mean MSE
mean_mse = mse_scores.mean()
print(f"\nMean MSE for XGBoost: {mean_mse}")

# Step 5: Perform Cross Validation for MAE
mae_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')

# Step 6: Print MAE scores for each fold
print("\nMAE scores for XGBoost:")
for i, mae_score in enumerate(mae_scores):
    print(f"Fold {i+1}: {mae_score}")

# Step 7: Calculate mean MAE
mean_mae = mae_scores.mean()
print(f"\nMean MAE for XGBoost: {mean_mae}")

"""### Random Forest"""

# Step 1: Train Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)
rf_model.fit(X_train, y_train)

# Step 2: Make predictions
y_pred = rf_model.predict(X_test)

# Step 3: Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Step 4: Print evaluation metrics
print(f"MSE: {mse}")
print(f"MAE: {mae}")

num_cols = len(X_test.columns) - 4  # Exclude the last 4 columns
num_rows = int(np.ceil(num_cols / 5))  # Number of rows to accommodate 5 plots per row

fig, axs = plt.subplots(num_rows, 5, figsize=(20, 4 * num_rows))

for i, column in enumerate(X_test.columns[:-4]):
    # Calculate the row and column index for the current subplot
    row_index = i // 5
    col_index = i % 5

    # Plot the actual data points
    axs[row_index, col_index].scatter(X_test[column], y_test, color='blue', label='Actual')

    # Plot the regression line
    axs[row_index, col_index].plot(X_test[column], y_pred, color='pink', label='Linear Regression')

    axs[row_index, col_index].set_title(f'Linear Regression for {column}')
    axs[row_index, col_index].set_xlabel(column)
    axs[row_index, col_index].set_ylabel('y')
    axs[row_index, col_index].legend()

# Hide empty subplots
for i in range(num_cols, num_rows * 5):
    axs.flatten()[i].axis('off')

plt.tight_layout()
plt.show()

for column in X_test.columns:
    # Plot the actual data points
    plt.scatter(X_test[column], y_test, color='blue', label='Actual')

    # Plot the regression line
    plt.plot(X_test[column], y_pred, color='pink', label='Random Forest Regression')

    plt.title(f'Random Forest Regression for {column}')
    plt.xlabel(column)
    plt.ylabel('y')
    plt.legend()
    plt.show()

"""#### Cross Validation"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Initialize Random Forest model
model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)

# Step 2: Perform Cross Validation for MSE
mse_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

# Step 3: Print MSE scores for each fold
print("MSE scores for Random Forest:")
for i, mse_score in enumerate(mse_scores):
    print(f"Fold {i+1}: {mse_score}")

# Step 4: Calculate mean MSE
mean_mse = mse_scores.mean()
print(f"\nMean MSE for Random Forest: {mean_mse}")

# Step 5: Perform Cross Validation for MAE
mae_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')

# Step 6: Print MAE scores for each fold
print("\nMAE scores for Random Forest:")
for i, mae_score in enumerate(mae_scores):
    print(f"Fold {i+1}: {mae_score}")

# Step 7: Calculate mean MAE
mean_mae = mae_scores.mean()
print(f"\nMean MAE for Random Forest: {mean_mae}")

from sklearn.metrics import r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Create polynomial features
degree = 2  # Set the degree of the polynomial
poly = PolynomialFeatures(degree)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Step 2: Train Polynomial Regression model
model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
model.fit(X_train_poly, y_train)

# Step 3: Make predictions
y_pred = model.predict(X_test_poly)

# Step 4: Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Step 5: Print evaluation metrics
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"R^2: {r2}")